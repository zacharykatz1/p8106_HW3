---
title: "P8106: Data Science II, Homework #3"
author: 'Zachary Katz (UNI: zak2132)'
date: "3/25/2022"
output: 
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: 3
  header-includes:
    -\usepackage{fancyhdr}
    -\usepackage{lipsum}
    -\pagestyle{fancy}
    -\fancyhead[R]{\thepage}
    -\fancypagestyle{plain}{\pagestyle{fancy}}
---

## Set-Up and Data Preprocessing

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(viridis)
library(Seurat)
library(AppliedPredictiveModeling)
library(caret)
library(glmnet)
library(mlbench)
library(pROC)
library(pdp)
library(vip)
library(klaR)
library(ggcorrplot)
library(MASS)
library(ggROC)
library(reshape2)
library(ggridges)
library(DataExplorer)

# Set global options for embedding plots and choosing themes
knitr::opts_chunk$set(warning = FALSE, message = FALSE, fig.align = "center")

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

```{r}
set.seed(77)

# Load data, clean column names, eliminate rows containing NA entries
data = read_csv("./Data/auto.csv") %>% 
  janitor::clean_names() %>% 
  na.omit() %>% 
  distinct() %>% 
  mutate(
    cylinders = as.factor(cylinders),
    year = as.factor(year),
    origin = case_when(origin == "1" ~ "American",
                       origin == "2" ~ "European",
                       origin == "3" ~ "Japanese"),
    origin = as.factor(origin),
    mpg_cat = as.factor(mpg_cat),
    mpg_cat = fct_relevel(mpg_cat, "low")
  ) %>% 
  as.data.frame()

# Partition data into training/test sets (70% split)
indexTrain = createDataPartition(y = data$mpg_cat,
                                 p = 0.7,
                                 list = FALSE)
```

## Part (a): Exploratory Data Analysis

```{r}
# Summary statistics
summary(data)
skimr::skim_without_charts(data)
```

We have 392 observations with 8 parameters: 7 predictors, including 4 continuous variables (`displacement`, `horsepower`, `weight`, `acceleration`) and 3 categorical variables (`cylinders`, `year`, `origin`), along with one binary outcome variable, `mpg_cat`, which takes values "high" and "low." Half our observations have the "high" label while the other half have the "low" label.  

```{r}
# Simple visualizations of the data

# Feature plot for all data (training and test), continuous predictors only
theme1 = transparentTheme(trans = 0.4)
trellis.par.set(theme1)

featurePlot(x = data %>% dplyr::select(horsepower, displacement, acceleration, weight),
            y = data$mpg_cat,
            scales = list(x = list(relation = "free"),
                          y = list(relation = "free")),
            plot = "density", pch = "|",
            auto.key = list(columns = 2))

# Alternative view: Distributions of continuous variables factored by outcome
data %>% 
  dplyr::select(displacement, horsepower, weight, acceleration, mpg_cat) %>% 
  melt(id.vars= "mpg_cat") %>% 
  ggplot(aes(x = value, y = mpg_cat)) + 
  stat_density_ridges(aes(color = mpg_cat, fill = mpg_cat), alpha = 0.2, quantile_lines = TRUE, quantiles = 2, jittered_points = TRUE) + 
  facet_wrap(~variable, scales = "free", nrow = 2) + 
  labs(x = "",
        y = "Density",
       fill = "MPG Class",
       color = "MPG Class")

# Distributions of categorical variables factored by outcome
data %>% 
  dplyr::select(-displacement, -horsepower, -weight, -acceleration) %>% 
  melt(id.vars = "mpg_cat") %>% 
  ggplot(aes(x = value, fill = mpg_cat)) + 
  geom_bar(position = "fill") + 
  facet_wrap(~variable, scales = "free", nrow = 2) + 
  scale_y_continuous(labels = scales::percent) + 
  labs(x = "",
        y = "Proportion by MPG Category",
       fill = "MPG Category",
       color = "MPG Category")

# Correlation plot for all data
model.matrix(~0+., data = data) %>% 
  cor(use = "pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type = "lower", lab = TRUE, lab_size = 2)

# LDA partition plots (continuous vars only), all data
partimat(mpg_cat ~ displacement + horsepower + weight + acceleration, method = "lda", data = data)
```

We conduct a few basic exploratory analyses. First, our feature plot of continuous covariates shows that cars with high MPG tend to have lower displacement, lower horsepower, lower weight, and higher acceleration. Similarly, looking at categorical covariates, we find that cars with higher MPG tend to have 4 or 5 cylinders, come from the 1980s (rather than 1970s), and be European or Japanese rather than American. From the correlation plot, we see that high MPG has the most positive correlation with the indicator for having 4 cylinders, and the most negative correlation with weight, displacement, and horsepower. There may also be some collinearity between these three continuous variables, potentially leading to some redundancy in the model. Finally, from the partition plots using LDA, we see how we would partition the classes based on every combination of two variables (continuous only), giving us the decision boundary. Red points are considered misclassified. Our error rate is lowest for the following combinations of two predictors: `horsepower` and `displacement`, and `acceleration` and `displacement`. On the other hand, our error rate is highest for `acceleration` and `horsepower`. (Note that we exclude factor variables from this analysis because the decision boundaries would be somewhat misleading.)

## Part (b): Logistic Regression

```{r}
set.seed(2132)

# Logistic regression using the training data (note: not using penalized logistic regression in this case): predict.glm
glm.fit = glm(mpg_cat ~ .,
              data = data,
              subset = indexTrain,
              family = binomial(link = "logit"))

# Check for statistically significant predictors
summary(glm.fit)
```

Here, we build a logistic regression model (without penalization) from our training data. At the 0.05 significance level, `weight`, `horsepower`, and `year79` are significant predictors of our outcome `mpg_cat`. At the 0.01 significance level, i.e. even more significantly, our indicator variable `year81` is a statistically significant predictor of our outcome as well.

```{r warning=FALSE, message=FALSE}
# Check performance on test data (use simple classifier with cut-off of 0.5)
test.pred.prob = predict(glm.fit, newdata = data[-indexTrain,],
                           type = "response")

test.pred = rep("low", length(test.pred.prob))

test.pred[test.pred.prob>0.5] = "high"

confusionMatrix(data = as.factor(test.pred),
                reference = data$mpg_cat[-indexTrain],
                positive = "high")
```

Our confusion matrix shows that our accuracy, or overall fraction of correct predictions, is roughly 92% (95% CI: 86% to 96%) once our model is applied to test data. The confusion matrix also tells us that our no information rate is 50%, which means that if we had no information and made the same class prediction for all observations, our model would be 50% accurate. Our p-value near 0 tells us that our accuracy is statistically significantly better than our no information rate. The model' is 96.7% sensitive (true detected positives out of all actual positives) and 87.9% specific (true detected negatives out of all actual negatives), with a positive predictive value of 88.9% (true detected positives out of all predicted positives) and a negative predictive value of 96.2% (true detected negatives out of all predicted negatives). Our sensitivity and specificity average to 92.2%, which is our balanced accuracy. Our kappa, at 0.8448, means that our inter-rater agreement is quite high, even accounting for the possibility of agreement by chance.

## Part (c): MARS Model

```{r}
# Train MARS model using the training data
set.seed(2132)

ctrl = trainControl(method = "repeatedcv",
                    summaryFunction = twoClassSummary,
                    repeats = 5,
                    classProbs = TRUE)

model.mars = train(x = data[indexTrain, 1:7],
                   y = data$mpg_cat[indexTrain],
                   method = "earth",
                   tuneGrid = expand.grid(degree = 1:3,
                                          nprune = 2:25),
                   metric = "ROC",
                   trControl = ctrl)

summary(model.mars)

ggplot(model.mars, highlight = T)

model.mars$bestTune %>% knitr::kable()

coef(model.mars$finalModel) %>% knitr::kable(col.names = "Coefficient")

vip(model.mars$finalModel)
```

Overall, our MARS model tells us that `cylinders4` (indicator for having 4 cylinders) is the most important  variable, with continuous variable `weight` and indicators `year82`, `year80`, and `year72` following closely behind, based on the overall impact of each variable on our regression function following a backward elimination procedure. Using `earth`, our model selects 14 out of 26 terms, representing 8 of 22 predictors (nprune terms = 15, product degree = 1). The model is optimized with and has an R-squared of 0.769.

## Part (d): LDA

```{r}
# LDA using the training data
lda.fit = lda(mpg_cat ~ ., data = data, subset = indexTrain)

# Plot the linear discriminants from LDA
plot(lda.fit, col = as.numeric(data$mpg_cat), abbrev = TRUE)

# Obtain scaling matrix
lda.fit$scaling
```

LDA has no tuning parameters, and allows us to classify by nearest centroid. Because we have two classes, we have k = 2-1 = 1 linear discriminants, and so our linear discriminant plot gives us the histogram of our transformed X (predictors) for both classes. In this case, when our "X" is lower, we tend to classify in the high `mpg_cat` group, whereas when our "X" is higher, we tend to classify in the low `mpg_cat` group. Finally, the scaling object gives us our matrix A, which is (k-1) x p matrix, or in this case, a simple column vector with one entry per predictor, given we only have two outcome classes. This matrix allows us to build our x-tilde (which is AX, a product of our transformation matrix and original predictors) for each observation / data point.

```{r}
# Alternatively, use caret for LDA
set.seed(2132)
    
training_df = data[indexTrain, ]

model.lda = train(mpg_cat ~ .,
                  data = training_df,
                  method = "lda",
                  metric = "ROC",
                  trControl = ctrl)

model.lda$results
```

For completeness, we also run an LDA model using `caret`, which has a 0.958 ROC, with 84% sensitivity and 97% specificity. 

## Part (e): Model Comparison and AUC/ROC

```{r}
# Model comparison based on ROC (training data)

# Run caret logistic model
set.seed(2132)

glm.logit.caret = train(x = data[indexTrain, 1:7],
                        y = data$mpg_cat[indexTrain],
                        method = "glm",
                        metric = "ROC",
                        trControl = ctrl)

res = resamples(list(LOGISTIC = glm.logit.caret,
                     MARS = model.mars,
                     LDA = model.lda))

summary(res)

bwplot(res, metric = "ROC")
```

Based on resampling / general cross-validation from how our models perform on the training data, having not seen the test data, I would choose the LDA model for classification of our response variable `mpg_cat`, as it has the highest ROC.

```{r}
# Predictions and ROC
lda.predict = predict(model.lda, newdata = data[-indexTrain, 1:7], type = "prob")[,2]

roc.lda = roc(data$mpg_cat[-indexTrain], lda.predict)

# Report AUC and misclassification rate
auc_lda = roc.lda$auc[1]

auc_lda

# Obtain classes
lda_class = lda.predict %>% 
  as.data.frame() %>% 
  mutate(
    class = case_when(. < 0.50 ~ "low",
                      . > 0.50 ~ "high")
  ) %>% 
  dplyr::select(class) %>% 
  as.matrix()

# Confusion matrix and misclassification error rate
confusionMatrix(data = as.factor(lda_class),
                reference = data$mpg_cat[-indexTrain],
                positive = "high")

# Plot ROC curve for best model (LDA)
modelName = "LDA model"

pROC::ggroc(list(roc.lda), legacy.axes = TRUE) + 
  scale_color_discrete(labels = paste0(modelName, " (", round(auc_lda, 2),")"),
                       name = "Model Type (AUC)") + 
  geom_abline(intercept = 0, slope = 1, color = "grey")
```

When applied to the previously unseen test data, the LDA model has a misclassification rate of 1 - 0.9397, or ~6%, when we use a threshold of 0.5 probability, as well as an AUC of 0.989, as observed on our ROC plot above.
